# This is a a prefilled template for `kwaak`
#
# Several assumptions and defaults have been filled in. For proper usage, please customize the values to your needs.
project_name = "avogadro"
language = "C++"

## If you are using OpenAI, set the api key here
#openai_api_key = "env:OPENAI_API_KEY"


## If you are using Anthropic, set the api key here
#anthropic_api_key = "env:ANTHROPIC_API_KEY"


## Optional: Connect kwaak to github to create PRs, search code, and automatically push to a  remote
#github_api_key = "env:GITHUB_TOKEN"


## Optional: Connect kwaak to tavily to enable it to search the web
#tavily_api_key = "env:TAVILY_API_KEY"

## Commands the agent uses for tools
## Kwaak can use tests, coverage, and lints to verify generated code.
## At the moment, the format of the output does not matter.
[commands]
## Optional: Allows an agent to run tests. Recommended.
# Example: test = "cargo test --no-fail-fast --color=never"
test = "ctest --test-dir build --output-on-failure"
## Optional: Allows an agent to run coverage. The coverage command should run the tests and output the coverage results to stdout.
# Example: coverage = "cargo llvm-cov --no-clean --summary-only"
#coverage = "<YOUR COVERAGE COMMAND>"
## Optional: Lint and fix command. This command is run after each completion cycle, before committing the code.
# Recommended to use, as it avoids the LLM getting distracted by linting issues
# Example: lint_and_fix = "cargo clippy --fix --allow-dirty --allow-staged && cargo fmt"
#lint_and_fix = "<YOUR LINT AND FIX COMMAND>"

## Git and GitHub configuration
#
## Kwaak can create and update PRs on Github, search github code, and interact with the git repository. This requires a github token.
## If you leave the token empty, kwaak will not create PRs.
[git]
main_branch = "master"
owner = "thosoo"
repository = "avogadro"
auto_push_remote = false

## Kwaak uses different LLMs for different tasks. As a rule of thumb, tasks that happen often (like indexing, summarizing) require a small, fast model
## and tasks that happen less often (like completion) can use a larger, more accurate model.
#
## You can overwrite the api key and base url per kind of task if needed.
[llm.indexing]
provider = "Ollama"
prompt_model = "codegemma:2b-code-v1.1-q4_K_M"
embedding_model = {name = "bge-m3", vector_size = 1024}
[llm.query]
provider = "Ollama"
prompt_model = "phi3-mini-tools"
[llm.embedding]
provider = "Ollama"
embedding_model = {name = "bge-m3", vector_size = 1024}
## Docker configuration
## kwaak requires a Dockerfile for the tool execution environment.
## Besides the dependencies to run the code, there are several additional dependencies:
## - `git` for interacting with the codebase
## - `rg` (ripgrep) for searching the codebase
## - `fd` (fd) for effective file searching
##
## In the future, an executor is planned that does not have these dependencies, but for now, they are required.
##
## If your project already has a Dockerfile and you want to keep it clean, you can specify a different file to use.
[docker]
dockerfile = "Dockerfile"
